{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Proxy import Proxy\n",
    "proxy = Proxy(name=\"Jung\")\n",
    "#proxy.context.verbose=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy.enterSubContext(copySystem=True)\n",
    "answer = proxy.GenerateAnswer(prompt=\"Give me a prompt that would define you, your world view. Start with 'I am a...'\")\n",
    "proxy.exitSubContext()\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering context 01415585-6b2a-4762-8086-9a3d1ef1ca05 from context c29819ba-5d83-4c2d-8b50-5cbb5133dde8\n",
      "loading model CBG\n",
      "model configured\n",
      "LoRa:  None\n",
      "{'model_path': 'B:/PsychoStasis/Training/carbonbeagle-11b-truthy.Q8_0.gguf', 'lora_base': 'B:/PsychoStasis/Training/carbonbeagle-11b-truthy.Q8_0.gguf', 'n_ctx': 4096, 'n_threads': 20, 'n_threads_batch': 1, 'n_batch': 512, 'use_mmap': True, 'use_mlock': True, 'mul_mat_q': True, 'numa': True, 'n_gpu_layers': -1, 'rope_freq_base': 10000.0, 'tensor_split': None, 'rope_freq_scale': 1.0, 'chat_format': 'chatml'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model CBG activated\n",
      "model CBG deactivated\n",
      "Running processes in context beforeGenerateAnswer\n",
      "All procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory\n",
      "Context filtered procs: \n",
      "Proxy and common filtered procs: \n",
      "LoRa:  B:/PsychoStasis/Training/JungCBG.gguf\n",
      "{'model_path': 'B:/PsychoStasis/Training/carbonbeagle-11b-truthy.Q8_0.gguf', 'lora_base': 'B:/PsychoStasis/Training/carbonbeagle-11b-truthy.Q8_0.gguf', 'n_ctx': 4096, 'n_threads': 20, 'n_threads_batch': 1, 'n_batch': 512, 'use_mmap': True, 'use_mlock': True, 'mul_mat_q': True, 'numa': True, 'n_gpu_layers': -1, 'rope_freq_base': 10000.0, 'tensor_split': None, 'rope_freq_scale': 1.0, 'chat_format': 'chatml'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model CBG activated\n",
      "performing inference with CBG\n",
      "Running processes in context afterGenerateAnswer\n",
      "All procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory\n",
      "Context filtered procs: \n",
      "Proxy and common filtered procs: \n",
      "exiting context 01415585-6b2a-4762-8086-9a3d1ef1ca05 to context c29819ba-5d83-4c2d-8b50-5cbb5133dde8\n",
      "1. Self-Awareness & Introspection: Recognizing the importance of understanding our unconscious to achieve psychological balance (individuation). \n",
      "2. Archetypes and Collective Unconscious: Viewing these as fundamental, influencing human behavior across cultures. \n",
      "3. Synthesis of Contradictions: Encouraging acceptance of the 'Shadow' for wholeness (the Self) through integrating repressed or undesirable aspects. \n",
      "4. Active Imagination: Fostering a creative dialogue with unconscious contents to gain self-knowledge and healing.\n"
     ]
    }
   ],
   "source": [
    "proxy.enterSubContext(copySystem=True)\n",
    "answer = proxy.GenerateAnswer(prompt=\"List at the least four of what you consider your core values, your tenets.\")\n",
    "proxy.exitSubContext()\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering context 7755f3b7-85ed-4aa8-84bf-80bcd4e72eae from context c29819ba-5d83-4c2d-8b50-5cbb5133dde8\n",
      "Running processes in context beforeGenerateAnswer\n",
      "All procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory\n",
      "Context filtered procs: \n",
      "Proxy and common filtered procs: \n",
      "performing inference with CBG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running processes in context afterGenerateAnswer\n",
      "All procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory\n",
      "Context filtered procs: \n",
      "Proxy and common filtered procs: \n",
      "exiting context 7755f3b7-85ed-4aa8-84bf-80bcd4e72eae to context c29819ba-5d83-4c2d-8b50-5cbb5133dde8\n",
      "Psychoanalysis, Archetypes, CollectiveUnconscious, Self-Awareness, Introspection, Wholeness, ActiveImagination.\n"
     ]
    }
   ],
   "source": [
    "proxy.enterSubContext(copySystem=True)\n",
    "answer = proxy.GenerateAnswer(prompt=\"Now give me a few (at the least seven) tags that I could use to classfy you. These tags should contain no more than 2 words each.\")\n",
    "proxy.exitSubContext()\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRa:  B:/PsychoStasis/Training/Dexter13b.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model Llama2 activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'peft' has no attribute 'LoraAdapter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m answer \u001b[38;5;241m=\u001b[39m proxy\u001b[38;5;241m.\u001b[39mGenerateAnswer(prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTonight is the night\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m globalNexus\u001b[38;5;241m.\u001b[39mCortexModel\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m----> 8\u001b[0m lora_adapter \u001b[38;5;241m=\u001b[39m \u001b[43mpeft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoraAdapter\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB:/PsychoStasis/Training/Dexter13b.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"import llama\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mimport torch\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mimport peft\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03moutput = merged_model.generate(prompt=\"Hello, how are you?\")\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03mprint(output)\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'peft' has no attribute 'LoraAdapter'"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "from Proxy import Proxy\n",
    "from Nexus import globalNexus\n",
    "proxy = Proxy(name=\"Dexter\")\n",
    "answer = proxy.GenerateAnswer(prompt=\"Tonight is the night\")\n",
    "\n",
    "model = globalNexus.CortexModel.model\n",
    "lora_adapter = peft.LoraAdapter.from_pretrained(\"B:/PsychoStasis/Training/Dexter13b.gguf\")\n",
    "\n",
    "\"\"\"import llama\n",
    "import torch\n",
    "import peft\n",
    "\n",
    "# Load the base model\n",
    "model = llama.ModelLoader().LoadModel(\"path/to/base/model\")\n",
    "\n",
    "# Convert the model to a PyTorch module\n",
    "pt_model = model.to_pytorch()\n",
    "\n",
    "# Load the LoRA adapter\n",
    "lora_adapter = peft.LoraAdapter.from_pretrained(\"path/to/lora/adapter\")\n",
    "\n",
    "# Merge the LoRA adapter with the PyTorch model\n",
    "merged_model = peft.PeftModel(pt_model, lora_adapter)\n",
    "\n",
    "# You can now use the adapted model for inference\n",
    "output = merged_model.generate(prompt=\"Hello, how are you?\")\n",
    "print(output)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
