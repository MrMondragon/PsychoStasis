{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Proxy import Proxy\n",
    "\n",
    "activeProxy = Proxy(\"Husserl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering context 8a1a53ee-43f0-43bb-ac57-eaaaf44ef318 from context 2769eec7-d8fd-4e58-a564-77d49a466033\n",
      "LoRa:  B:/PsychoStasis/Training/HusserlCBG.gguf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m activeProxy\u001b[38;5;241m.\u001b[39menterSubContext(deepCopy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mactiveProxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerateAnswer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the topic of this conversation? Define it in 4 words or less.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m activeProxy\u001b[38;5;241m.\u001b[39mexitSubContext()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "File \u001b[1;32mb:\\PsychoStasis\\Proxy.py:105\u001b[0m, in \u001b[0;36mProxy.GenerateAnswer\u001b[1;34m(self, prompt, shard, contextCallback, innerThoughts, grammar, max_tokens, recusiveLevel)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(shard\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    103\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadLora();   \n\u001b[1;32m--> 105\u001b[0m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m context\u001b[38;5;241m.\u001b[39mwindowSize \u001b[38;5;241m=\u001b[39m globalNexus\u001b[38;5;241m.\u001b[39mCortexModel\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_ctx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m globalNexus\u001b[38;5;241m.\u001b[39mCortexModel\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]      \n\u001b[0;32m    107\u001b[0m context\u001b[38;5;241m.\u001b[39mproxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mb:\\PsychoStasis\\.\\BaseModel.py:53\u001b[0m, in \u001b[0;36mBaseModel.activate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactivate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive):\n\u001b[1;32m---> 53\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodelName\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m activated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mb:\\PsychoStasis\\.\\GGUFModel.py:142\u001b[0m, in \u001b[0;36mGGUFModel.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    122\u001b[0m     tensor_split_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor_split\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)]            \n\u001b[0;32m    124\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath),\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlora_base\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_format\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_format\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    140\u001b[0m }            \n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m Llama(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(cache_capacity \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\marce\\.conda\\envs\\stasis\\lib\\site-packages\\llama_cpp_cuda\\llama.py:922\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[1;34m(self, model_path, n_gpu_layers, main_gpu, tensor_split, vocab_only, use_mmap, use_mlock, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_scaling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, mul_mat_q, logits_all, embedding, offload_kqv, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43m_LlamaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# Set the default value for the context and correct the batch\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_ctx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\marce\\.conda\\envs\\stasis\\lib\\site-packages\\llama_cpp_cuda\\llama.py:236\u001b[0m, in \u001b[0;36m_LlamaModel.__init__\u001b[1;34m(self, path_model, params, verbose)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress_stdout_stderr(disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose):\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_LoadModel_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marce\\.conda\\envs\\stasis\\lib\\site-packages\\llama_cpp_cuda\\llama_cpp.py:652\u001b[0m, in \u001b[0;36mllama_LoadModel_from_file\u001b[1;34m(path_model, params)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_LoadModel_from_file\u001b[39m(\n\u001b[0;32m    650\u001b[0m     path_model: \u001b[38;5;28mbytes\u001b[39m, params: llama_model_params\n\u001b[0;32m    651\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m llama_model_p:\n\u001b[1;32m--> 652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_LoadModel_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "activeProxy.enterSubContext(deepCopy=True)\n",
    "answer = activeProxy.GenerateAnswer(prompt=\"What is the topic of this conversation? Define it in 4 words or less.\")\n",
    "activeProxy.exitSubContext()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CommitToSummaryMemory': <CognitiveProcesses.CommitToSummaryMemory.CommitToSummaryMemory at 0x1c505beedf0>,\n",
       " 'CommitToEpisodicMemory': <CognitiveProcesses.CommitToEpisodicMemory.CommitToEpisodicMemory at 0x1c505beed60>,\n",
       " 'CommitToAbstractMemory': <CognitiveProcesses.CommitToAbstractMemory.CommitToAbstractMemory at 0x1c505beee80>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from CognitiveSystem import cognitiveSystem\n",
    "cognitiveSystem.processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cognitiveSystem.RunProcess(proxy = activeProxy, procName = \"CommitToEpisodicMemory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering context 04997d00-bd5c-47c0-92d9-005a6f81d34f from context 173c8650-3849-487b-8be2-0f283de6fbe8\n",
      "LoRa:  B:/PsychoStasis/Training/HusserlCBG.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "from_string grammar:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model CBG activated\n",
      "performing inference with CBG\n",
      "root ::= root_2 \n",
      "item ::= [-] [ ] item_3 [<U+000A>] \n",
      "root_2 ::= item root_2 | item \n",
      "item_3 ::= [^<U+000D><U+000A><U+000B><U+000C><U+0085><U+2028><U+2029>] item_3 | [^<U+000D><U+000A><U+000B><U+000C><U+0085><U+2028><U+2029>] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at B:/PsychoStasis/models/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context commited\n",
      "exiting context 04997d00-bd5c-47c0-92d9-005a6f81d34f to context 173c8650-3849-487b-8be2-0f283de6fbe8\n",
      "model NER.NER activated\n",
      "model NER.NER deactivated\n",
      "model Embeddings.Embeddings activated\n",
      "model Embeddings.Embeddings deactivated\n",
      "model Embeddings.Embeddings activated\n",
      "model Embeddings.Embeddings deactivated\n",
      "context commited\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"- The user's ongoing interest in AI, consciousness, and phenomenology is apparent.\",\n",
       " '- Intersubjectivity as a central theme reaffirms its significance to our discussions.',\n",
       " \"- Husserl's skepticism about machine sentience due to the absence of subjective experience remains consistent.\",\n",
       " \"- The 'ego', the distinction between biological and artificial systems, and the fictionality of this construct are recurring topics.\",\n",
       " '- Memory, erasure, and the nature of our philosophical engagement as an intellectual exercise persist.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cognitiveSystem.RunProcess(proxy = activeProxy, procName = \"CommitToAbstractMemory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Split Commit to memory to 3 </br>\n",
    "- [ ] Test episodic memory </br>\n",
    "- [ ] Test commit to consolidated memory </br>\n",
    "- [ ] Test commit to Abstract memory </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldcontext = activeProxy.context\n",
    "oldcontext.commitedProcesses[\"CommitToAbstractMemory\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['99dec728-545c-4455-9001-b3e709b4b9b3']],\n",
       " 'distances': [[1.1003241957929113]],\n",
       " 'metadatas': [[{'conversationId': '173c8650-3849-487b-8be2-0f283de6fbe8',\n",
       "    'proxy': 'Husserl',\n",
       "    'timestamp': 1710965320}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [[\"- The user's ongoing interest in AI, consciousness, and phenomenology is apparent.\"]],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Memory import longTermMemory\n",
    "from Nexus import globalNexus\n",
    "\n",
    "globalNexus.BeginShardBatch(\"Embeddings.Embeddings\")\n",
    "longTermMemory.AbstractMemory.get(None)\n",
    "\n",
    "queryResult =longTermMemory.AbstractMemory.query(query_texts=[\"Artificial Intelligence\"], n_results=1,\n",
    "                                 where={\"conversationId\":'173c8650-3849-487b-8be2-0f283de6fbe8'})\n",
    "\n",
    "queryResult"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
