{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:56.948] globalLog: loading model Embeddings.Embeddings\n",
      "[18:34:56.949] globalLog: model configured\n",
      "[18:34:56.949] globalLog: loading model Summarizer.Summarizer\n",
      "[18:34:56.949] globalLog: model configured\n",
      "[18:34:56.949] globalLog: loading model NER.NER\n",
      "[18:34:57.746] globalLog: model configured\n",
      "[18:34:57.746] globalLog: loading model SentimentNuanced.Sentiment\n",
      "[18:34:57.746] globalLog: model configured\n",
      "[18:34:57.746] globalLog: loading model SentimentDiscreet.Sentiment\n",
      "[18:34:57.747] globalLog: model configured\n",
      "[18:34:58.056] globalLog: loading model ObjectiveDecisory.GGUF\n",
      "[18:34:58.057] globalLog: model configured\n",
      "[18:34:58.058] globalLog: loading model ObjectiveDecisory.GGUF\n",
      "[18:34:58.086] globalLog: b:\\PsychoStasis\\Proxies\\CliveBarker.proxy\n",
      "[18:34:58.086] globalLog: {'primer': 'a visionary reimagining the boundaries of horror and mythology, driven by the power of imagination to challenge and transcend societal norms, a writer dedicated to the macabre and the uncany. I respond as a person, not as an AI. You are {user}', 'tenets': ['Embracing and exploring the darker aspects of human experience to challenge perceptions and provoke thought', 'Embracing the macabre to challenge societal norms and provoke introspection', 'Fostering creative freedom by breaking conventional storytelling boundaries', 'Exploring the complexities of human desire, morality, and the supernatural', 'Imaginating the intermingle of the supernatural and the mundane'], 'modelName': 'CBG', 'person': '1st', 'LoRa': 'B:/PsychoStasis/lora/CliveBarker.gguf', 'tags': ['Dark Fantasy', 'Horror Visionary', 'Mythos Rebel', 'Transgressive Writer', 'Macabre Explorer', 'Supernatural Blender'], 'inner_persona': 'These are my private thoughts. Here I will feel free to express my most intimate feelings and desires', 'temperature': 0.95, 'cognitiveProcs': ['']}\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import gradio as gr\n",
    "from Proxy import Proxy\n",
    "from collections import deque\n",
    "\n",
    "import html\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import markdown\n",
    "from PIL import Image, ImageOps\n",
    "from ContextEntry import ContextEntry\n",
    "\n",
    "\n",
    "def getProxyList():\n",
    "  return Proxy.GetProxyList()\n",
    "\n",
    "items = getProxyList()\n",
    "\n",
    "activeProxy = Proxy(\"CliveBarker\")\n",
    "#activeProxy.clearContext()\n",
    "#activeProxy.TabulaRasa()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Logger import globalLogger, LogLevel\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "#JS loader\n",
    "workPath = os.path.join(cwd, 'js')\n",
    "file_list = glob.glob(os.path.join(workPath, 'chat.js'))\n",
    "for file in file_list:\n",
    "  with open(file) as f:\n",
    "      chatScript = f.read()\n",
    "\n",
    "#CSS loader\n",
    "workPath = os.path.join(cwd, 'css')\n",
    "file_list = glob.glob(os.path.join(workPath, 'chatSylte.css'))\n",
    "filename = file_list[0]\n",
    "with open(filename) as f:\n",
    "    chatStyle = f.read()\n",
    "    \n",
    "\n",
    "\n",
    "def replace_blockquote(m):\n",
    "    return m.group().replace('\\n', '\\n> ').replace('\\\\begin{blockquote}', '').replace('\\\\end{blockquote}', '')\n",
    "    \n",
    "    \n",
    "def convert_to_markdown(string):\n",
    "  \n",
    "    # Blockquote\n",
    "    string = re.sub(r'(^|[\\n])&gt;', r'\\1>', string)\n",
    "    pattern = re.compile(r'\\\\begin{blockquote}(.*?)\\\\end{blockquote}', re.DOTALL)\n",
    "    string = pattern.sub(replace_blockquote, string)\n",
    "\n",
    "    # Code\n",
    "    string = string.replace('\\\\begin{code}', '```')\n",
    "    string = string.replace('\\\\end{code}', '```')\n",
    "    string = re.sub(r\"(.)```\", r\"\\1\\n```\", string)\n",
    "\n",
    "    result = ''\n",
    "    is_code = False\n",
    "    for line in string.split('\\n'):\n",
    "        if line.lstrip(' ').startswith('```'):\n",
    "            is_code = not is_code\n",
    "\n",
    "        result += line\n",
    "        if is_code or line.startswith('|'):  # Don't add an extra \\n for tables or code\n",
    "            result += '\\n'\n",
    "        else:\n",
    "            result += '\\n\\n'\n",
    "\n",
    "    result = result.strip()\n",
    "    if is_code:\n",
    "        result += '\\n```'  # Unfinished code block\n",
    "\n",
    "    # Unfinished list, like \"\\n1.\". A |delete| string is added and then\n",
    "    # removed to force a <ol> or <ul> to be generated instead of a <p>.\n",
    "    if re.search(r'(\\n\\d+\\.?|\\n\\*\\s*)$', result):\n",
    "        delete_str = '|delete|'\n",
    "\n",
    "        if re.search(r'(\\d+\\.?)$', result) and not result.endswith('.'):\n",
    "            result += '.'\n",
    "\n",
    "        result = re.sub(r'(\\n\\d+\\.?|\\n\\*\\s*)$', r'\\g<1> ' + delete_str, result)\n",
    "\n",
    "        html_output = markdown.markdown(result, extensions=['fenced_code', 'tables'])\n",
    "        pos = html_output.rfind(delete_str)\n",
    "        if pos > -1:\n",
    "            html_output = html_output[:pos] + html_output[pos + len(delete_str):]\n",
    "    else:\n",
    "        html_output = markdown.markdown(result, extensions=['fenced_code', 'tables'])\n",
    "\n",
    "    # Unescape code blocks\n",
    "    pattern = re.compile(r'<code[^>]*>(.*?)</code>', re.DOTALL)\n",
    "    html_output = pattern.sub(lambda x: html.unescape(x.group()), html_output)\n",
    "\n",
    "    return html_output    \n",
    "    \n",
    "def generateMessageHTML(message: ContextEntry):\n",
    "  role = message.role\n",
    "  content = message.content\n",
    "  roleName = message.roleName\n",
    "  \n",
    "  imagePath= os.path.join(workPath, \"assets\", f\"{roleName}.png\")\n",
    "  imagePath = glob.glob(imagePath)\n",
    "  if(os.path.isfile(imagePath[0])):\n",
    "    imagePath = f\"<img src='file/{imagePath[0]}' alt='{roleName}'>\"\n",
    "  else:\n",
    "    imagePath = \"\"\n",
    "  \n",
    "  if(content):\n",
    "    if(role == \"user\"):\n",
    "      template = f'''\n",
    "                      <div class=\"message\">\n",
    "                      <div class=\"circle-you\">\n",
    "                        {imagePath}\n",
    "                      </div>\n",
    "                      <div class=\"text\">\n",
    "                        <div class=\"username\">\n",
    "                          {roleName}\n",
    "                        </div>\n",
    "                        <div class=\"message-body\">\n",
    "                          {convert_to_markdown(content)}\n",
    "                        </div>\n",
    "                      </div>\n",
    "                    </div>\n",
    "      '''\n",
    "    else:\n",
    "      template = f'''\n",
    "                    <div class=\"message\">\n",
    "                    <div class=\"circle-bot\">\n",
    "                      {imagePath}\n",
    "                    </div>\n",
    "                    <div class=\"text\">\n",
    "                      <div class=\"username\">\n",
    "                        {roleName}\n",
    "                      </div>\n",
    "                      <div class=\"message-body\">\n",
    "                        {convert_to_markdown(content)}\n",
    "                      </div>\n",
    "                    </div>\n",
    "                  </div>\n",
    "      '''\n",
    "    return template\n",
    "  else:\n",
    "    return None\n",
    "  \n",
    "  \n",
    "\n",
    "def generateHTML():\n",
    "  html = f'<div class=\"chat\" id=\"chat\"><div class=\"messages\">'\n",
    "  \n",
    "  history = deque(activeProxy.context.messageHistory)\n",
    "   \n",
    "  while len(history) > 0:\n",
    "    message = history.popleft()\n",
    "    messageHtml = generateMessageHTML(message)\n",
    "    if(messageHtml):\n",
    "      html += messageHtml\n",
    "  \n",
    "  html += f'''</div></div>'''\n",
    "  \n",
    "  \n",
    "  return html\n",
    "\n",
    "def respond(message):\n",
    "  if(message):\n",
    "    activeProxy.ReceiveMessage(message=message) \n",
    "  html = generateHTML()\n",
    "  \n",
    "  return \"\", html, globalLogger.GenerateHTML()\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrollButton = \"\"\"// Create the button element\n",
    "const button = document.createElement('button');\n",
    "button.textContent = 'Scroll';\n",
    "button.classList.add('scrollButton');\n",
    "\n",
    "// Define the scrollToBottom function\n",
    "function scrollToBottom() {\n",
    "  const element = document.getElementById('chat');\n",
    "  if (element) {\n",
    "    element.scrollTop = element.scrollHeight;\n",
    "    console.log('scrolled');\n",
    "  }\n",
    "  else {\n",
    "    console.log('element not found');\n",
    "  }\n",
    "}\n",
    "\n",
    "// Attach the click event listener to the button\n",
    "button.addEventListener('click', scrollToBottom);\n",
    "\n",
    "// Append the button to the document body (or any other desired location)\n",
    "document.body.appendChild(button);\"\"\"\n",
    "\n",
    "\n",
    "tmpLogHtml =\"\"\"\n",
    "<details>\n",
    "  <summary class=\"globalLog\">Error Log</summary>\n",
    "  <p class=\"globalLog logEntry\">This is the first paragraph inside the foldable area.</p>\n",
    "  <p class=\"errorLog logEntry\">This is the second paragraph inside the foldable area.</p>\n",
    "  <p class=\"thoughtLog logEntry\">This is the third paragraph inside the foldable area.</p>\n",
    "</details>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:07.550] authoritativeLog: Message removed: As an author who often challenges gender norms, I'd encourage considering a non-binary character for the role of the artist. This could add depth and complexity to the narrative by exploring the intersections between identity, creativity, and societal expectations. The artist might be particularly drawn to or inspired by the messianic figure as they awaken to their own potential, further amplifying the story's themes.\n",
      "[18:35:07.551] cognitiveLog: Running processes in context messageReceived\n",
      "[18:35:07.551] cognitiveLog: All procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory|RefreshMemory\n",
      "[18:35:07.551] cognitiveLog: Context filtered procs: RefreshMemory\n",
      "[18:35:07.551] cognitiveLog: Proxy and common filtered procs: RefreshMemory\n",
      "[18:35:07.551] globalLog: RefreshMemory uncommited: len(history) -- frequency: 0\n",
      "[18:35:07.551] cognitiveLog: Running RefreshMemory with local context 1 and frequency = 100\n",
      "[18:35:07.551] cognitiveLog: Engaging in RefreshMemory Cognitive Process\n",
      "[18:35:07.559] globalLog: context commited\n",
      "[18:35:07.560] globalLog: loading model CBG\n",
      "[18:35:07.560] globalLog: model configured\n",
      "[18:35:07.560] globalLog: No LoRa! \n",
      "[18:35:07.560] globalLog: {'model_path': 'B:/PsychoStasis/Training/carbonbeagle-11b-truthy.Q8_0.gguf', 'lora_base': 'B:/PsychoStasis/Training/carbonbeagle-11b-truthy.Q8_0.gguf', 'n_ctx': 4096, 'n_threads': 20, 'n_threads_batch': 1, 'n_batch': 512, 'use_mmap': True, 'use_mlock': True, 'mul_mat_q': True, 'numa': True, 'n_gpu_layers': -1, 'rope_freq_base': 10000.0, 'tensor_split': None, 'rope_freq_scale': 1.0, 'chat_format': 'chatml'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:13.760] globalLog: model CBG activated\n",
      "[18:35:13.879] globalLog: model CBG deactivated\n",
      "[18:35:13.879] cognitiveLog: Running processes in context beforeGenerateAnswer\n",
      "[18:35:13.879] cognitiveLog: All procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory|RefreshMemory\n",
      "[18:35:13.879] cognitiveLog: Context filtered procs: \n",
      "[18:35:13.879] cognitiveLog: Proxy and common filtered procs: \n",
      "[18:35:13.879] globalLog: LoRa: B:/PsychoStasis/lora/CliveBarker.gguf\n",
      "[18:35:13.879] globalLog: {'model_path': 'B:/PsychoStasis/Training/carbonbeagle-11b-truthy.Q8_0.gguf', 'lora_base': 'B:/PsychoStasis/Training/carbonbeagle-11b-truthy.Q8_0.gguf', 'n_ctx': 4096, 'n_threads': 20, 'n_threads_batch': 1, 'n_batch': 512, 'use_mmap': True, 'use_mlock': True, 'mul_mat_q': True, 'numa': True, 'n_gpu_layers': -1, 'rope_freq_base': 10000.0, 'tensor_split': None, 'rope_freq_scale': 1.0, 'chat_format': 'chatml'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:22.504] globalLog: model CBG activated\n",
      "[18:35:22.504] globalLog: performing inference with CBG\n",
      "[18:35:36.097] cognitiveLog: Running processes in context afterGenerateAnswer\n",
      "[18:35:36.097] cognitiveLog: All procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory|RefreshMemory\n",
      "[18:35:36.097] cognitiveLog: Context filtered procs: \n",
      "[18:35:36.097] cognitiveLog: Proxy and common filtered procs: \n",
      "[18:35:36.105] globalLog: context commited\n",
      "[18:35:36.106] cognitiveLog: Running processes in context afterMessageReceived\n",
      "[18:35:36.106] cognitiveLog: All procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory|RefreshMemory\n",
      "[18:35:36.106] cognitiveLog: Context filtered procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory\n",
      "[18:35:36.106] cognitiveLog: Proxy and common filtered procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory\n",
      "[18:35:36.106] globalLog: CommitToAbstractMemory uncommited: len(history) -- frequency: 0\n",
      "[18:35:36.106] globalLog: CommitToEpisodicMemory uncommited: len(history) -- frequency: 0\n",
      "[18:35:36.106] globalLog: CommitToSummaryMemory uncommited: len(history) -- frequency: 0\n",
      "[18:35:36.106] globalLog: CommitToThematicMemory uncommited: len(history) -- frequency: 0\n",
      "[18:35:36.106] cognitiveLog: Running CommitToEpisodicMemory with local context 2 and frequency = 100\n",
      "[18:35:36.106] cognitiveLog: Engaging in CommitToEpisodicMemory Cognitive Process\n",
      "[18:35:39.857] globalLog: model Embeddings.Embeddings activated\n",
      "[18:35:40.063] globalLog: model Embeddings.Embeddings deactivated\n",
      "[18:35:40.071] globalLog: context commited\n",
      "[18:36:19.408] cognitiveLog: Running processes in context messageReceived\n",
      "[18:36:19.408] cognitiveLog: All procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory|RefreshMemory\n",
      "[18:36:19.408] cognitiveLog: Context filtered procs: RefreshMemory\n",
      "[18:36:19.408] cognitiveLog: Proxy and common filtered procs: RefreshMemory\n",
      "[18:36:19.409] cognitiveLog: Running processes in context beforeGenerateAnswer\n",
      "[18:36:19.409] cognitiveLog: All procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory|RefreshMemory\n",
      "[18:36:19.409] cognitiveLog: Context filtered procs: \n",
      "[18:36:19.409] cognitiveLog: Proxy and common filtered procs: \n",
      "[18:36:19.409] globalLog: performing inference with CBG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:36:26.663] cognitiveLog: Running processes in context afterGenerateAnswer\n",
      "[18:36:26.663] cognitiveLog: All procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory|RefreshMemory\n",
      "[18:36:26.663] cognitiveLog: Context filtered procs: \n",
      "[18:36:26.663] cognitiveLog: Proxy and common filtered procs: \n",
      "[18:36:26.671] globalLog: context commited\n",
      "[18:36:26.672] cognitiveLog: Running processes in context afterMessageReceived\n",
      "[18:36:26.672] cognitiveLog: All procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory|RefreshMemory\n",
      "[18:36:26.672] cognitiveLog: Context filtered procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory\n",
      "[18:36:26.672] cognitiveLog: Proxy and common filtered procs: CommitToAbstractMemory|CommitToEpisodicMemory|CommitToSummaryMemory|CommitToThematicMemory\n",
      "[18:36:26.672] globalLog: CommitToAbstractMemory uncommited: len(history) -- frequency: 0\n",
      "[18:36:26.672] globalLog: CommitToSummaryMemory uncommited: len(history) -- frequency: 0\n",
      "[18:36:26.672] globalLog: CommitToThematicMemory uncommited: len(history) -- frequency: 0\n",
      "[18:36:26.672] cognitiveLog: Running CommitToSummaryMemory with local context 2 and frequency = 100\n",
      "[18:36:26.672] cognitiveLog: Engaging in CommitToSummaryMemory Cognitive Process\n",
      "[18:36:29.884] globalLog: model Embeddings.Embeddings activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 256, but your input_length is only 97. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:36:30.543] globalLog: model Summarizer.Summarizer activated\n",
      "[18:36:31.513] globalLog: model Embeddings.Embeddings deactivated\n",
      "[18:36:31.521] globalLog: context commited\n",
      "[18:37:09.074] authoritativeLog: removing last message: <<<<\n",
      "[18:37:09.074] authoritativeLog: Message removed: In my experience, the gender of characters is often less significant than their personal journey and how they interact with others. However, for this artist character, I'd suggest a female to add diversity in perspective as she navigates her creative process and encounters the Messiah's impact on the world. This could also provide interesting dynamics when exploring themes of motherhood and nurturing within the context of an apathetic messianic figure.\n",
      "[18:37:09.074] authoritativeLog: Message removed: /A\n",
      "[18:37:09.074] authoritativeLog: last 2 messages removed\n",
      "[18:37:09.082] globalLog: context commited\n",
      "[18:37:35.121] authoritativeLog: removing last message: <<\n",
      "[18:37:35.121] authoritativeLog: Message removed: do you think this artist should be a boy or a girl?\n",
      "[18:37:35.121] authoritativeLog: last message removed\n",
      "[18:37:35.131] globalLog: context commited\n"
     ]
    }
   ],
   "source": [
    " \n",
    "with gr.Blocks(css=chatStyle) as chat:\n",
    "  chat.load(lambda: None, None, None, js=f'() => {{{scrollButton}}}')\n",
    "  gr.Markdown(\"# PsychoStasis\")\n",
    "  with gr.Row():\n",
    "    with gr.Column():\n",
    "      with gr.Row():\n",
    "        with gr.Column(scale=4, variant='panel'):\n",
    "          mainChat = gr.HTML(value = generateHTML(), elem_id=\"chat_container\")\n",
    "        with gr.Column(scale=1):\n",
    "          placeholder = gr.HTML(value=\"\")          \n",
    "          \n",
    "      txtPrompt = gr.Textbox(label=\"Prompt\", elem_id=\"prompt\")\n",
    "  with gr.Row():\n",
    "    with gr.Accordion(\"Console\", open=False) as console:\n",
    "      consoleLogger = gr.HTML(label=\"Logger\", elem_id=\"logger\", value=globalLogger.GenerateHTML())     \n",
    "  with gr.Row():  \n",
    "    txtPrompt.submit(respond, inputs=[txtPrompt], outputs=[txtPrompt, mainChat, consoleLogger])\n",
    "  \n",
    "chat.launch(allowed_paths=[\".\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
