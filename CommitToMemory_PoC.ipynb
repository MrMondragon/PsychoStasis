{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Proxy import Proxy\n",
    "proxy = Proxy(name=\"Ergo\")\n",
    "\n",
    "\n",
    "proxy.context.append_message(message=\"\"\"in c# we have a trim() method in the string class to remove trailing and starting empty spaces from a string. What is the correspondent method in python?\"\"\", role=\"user\")\n",
    "proxy.context.append_message(message=\"\"\"In Python, there isn't a built-in `trim()` method like in C#. However, you can achieve the same result by using the `strip()` function or `lstrip()` and `rstrip()` functions which are equivalent to `Trim()`, `TrimStart()`, and `TrimEnd()` respectively in C#.\n",
    "\n",
    "Here's how you would use them:\n",
    "\n",
    "```python\n",
    "str = \"   Hello World   \"\n",
    "print(str.strip())  # Removes leading and trailing spaces\n",
    "print(str.lstrip())  # Removes only leading spaces\n",
    "print(str.rstrip())  # Removes only trailing spaces\n",
    "```\"\"\", role=\"assistant\")\n",
    "\n",
    "proxy.context.append_message(message=\"\"\"give me a regular expression to detect the following: the string representation for numbers One to Ten and literal numbers, including decimals\"\"\", role=\"user\")\n",
    "proxy.context.append_message(message='''In Python, you can use the `re` module to work with Regular Expressions. Here's an example of how you might create a regular expression that matches strings representing numbers from one to ten (in words or digits) as well as decimal numbers:\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "def is_number(n):\n",
    "    number_pattern = r\"^(one|two|three|four|five|six|seven|eight|nine|ten|1[0-9]?|20|0\\.[1-9])$|^([0-9]{1,3}(,[0-9]{3})*(\\.[0-9]+)?)$\"\n",
    "    return bool(re.match(number_pattern, n, re.IGNORECASE))\n",
    "```\n",
    "This regular expression works as follows:\n",
    "\n",
    "- `^` asserts the start of a line.\n",
    "- `one|two|three|four|five|six|seven|eight|nine|ten` matches any of these words (case insensitive due to `re.IGNORECASE`).\n",
    "- `1[0-9]?|20` matches numbers from 10 to 20.\n",
    "- `0\\.[1-9]` matches decimal numbers between 0 and 1, excluding 0 itself (e.g., 0.5, 0.7).\n",
    "- `([0-9]{1,3}(,[0-9]{3})*(\\.[0-9]+)?)` matches any number from 0 to 999,999,999 with optional grouping of digits by thousands (e.g., 1,000,000) and decimal part.\n",
    "- `$` asserts the end of a line.\n",
    "\n",
    "this regular expression will match numbers in the range from 0 to 20 inclusive, as well as any number with up to three digits before the decimal point and one or more digits after it. If you need to match larger numbers, you can adjust the `{1,3}` part of the regular expression accordingly.\n",
    "```''', role=\"assistant\")\n",
    "\n",
    "proxy.context.append_message(message=\"\"\"now, considering I have this regular expression string loaded in a variable named pattern, show me how can I replace values in a string using this pattern regex\"\"\", role=\"user\")\n",
    "proxy.context.append_message(message='''You can use the `re.sub()` function to replace matched substrings with another substring. Here's an example of how you might do it:\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "# Your regular expression pattern\n",
    "pattern = r\"your_regular_expression_here\"\n",
    "\n",
    "# The string in which you want to make replacements\n",
    "text = \"your text here\"\n",
    "\n",
    "# The substring that will replace the matched substrings\n",
    "replacement = \"your replacement text here\"\n",
    "\n",
    "new_text = re.sub(pattern, replacement, text)\n",
    "```\n",
    "\n",
    "In this example, `re.sub()` will find all occurrences of the pattern in the input string and replace them with the specified replacement substring. The result is stored in `new_text`.\n",
    "\n",
    "Please note that you need to replace \"your_regular_expression_here\", \"your text here\" and \"your replacement text here\" with your actual regular expression, input string and replacement string respectively.''', role=\"assistant\")\n",
    "\n",
    "proxy.context.append_message(message=\"\"\"what arguments can I pass to the strip method in python?\"\"\", role=\"user\")\n",
    "proxy.context.append_message(message='''The `strip()` function in Python removes leading and trailing spaces from a string by default. However, you can also specify characters that should be removed using the `strip()` method. This is done by passing these characters as an argument to the `strip()` method.\n",
    "\n",
    "Here's how you can use it:\n",
    "\n",
    "```python\n",
    "str = \"---Hello World---\"\n",
    "print(str.strip('-'))  # Removes leading and trailing '-'\n",
    "```\n",
    "\n",
    "In this example, all hyphens at the beginning and end of the string are removed because they were passed as an argument to `strip()`.\n",
    "\n",
    "If you don't pass any arguments to `strip()`, it will remove spaces from both ends of the string:\n",
    "\n",
    "```python\n",
    "str = \"   Hello World    \"\n",
    "print(str.strip())  # Removes leading and trailing spaces\n",
    "```\n",
    "\n",
    "In this case, all spaces at the beginning and end of the string are removed because no arguments were passed to `strip()`.\n",
    "\n",
    "''', role=\"assistant\")\n",
    "\n",
    "proxy.context.append_message(message=\"\"\"what arguments can I pass to the strip method in python?\"\"\", role=\"user\")\n",
    "proxy.context.append_message(message='''The `strip()` function in Python removes leading and trailing spaces from a string by default. However, you can also specify characters that should be removed using the `strip()` method. This is done by passing these characters as an argument to the `strip()` method.\n",
    "\n",
    "Here's how you can use it:\n",
    "\n",
    "```python\n",
    "str = \"---Hello World---\"\n",
    "print(str.strip('-'))  # Removes leading and trailing '-'\n",
    "```\n",
    "\n",
    "In this example, all hyphens at the beginning and end of the string are removed because they were passed as an argument to `strip()`.\n",
    "\n",
    "If you don't pass any arguments to `strip()`, it will remove spaces from both ends of the string:\n",
    "\n",
    "```python\n",
    "str = \"   Hello World    \"\n",
    "print(str.strip())  # Removes leading and trailing spaces\n",
    "```\n",
    "\n",
    "In this case, all spaces at the beginning and end of the string are removed because no arguments were passed to `strip()`.\n",
    "\n",
    "''', role=\"assistant\")\n",
    "\n",
    "proxy.context.append_message(message=\"\"\"can I specify more than one character? or does this need to be done in different calls?\"\"\", role=\"user\")\n",
    "proxy.context.append_message(message='''You can pass multiple characters as an argument to the `strip()` method, but they will all be treated as a single string. Here's how you can do it:\n",
    "\n",
    "```python\n",
    "str = \"---Hello World---\"\n",
    "print(str.strip('-='))  # Removes leading and trailing '-' or '='\n",
    "```\n",
    "\n",
    "In this example, both hyphens (`-`) and equals signs (`=`) at the beginning and end of the string are removed because they were passed as an argument to `strip()`. The characters are treated as a single string, so if any character in the string matches one of these characters, it will be removed.\n",
    "\n",
    "If you want to remove different sets of characters from the start or end of the string, you would need to make separate calls to `strip()`:\n",
    "\n",
    "```python\n",
    "str = \"---Hello World===\"\n",
    "print(str.strip('-').strip('='))  # Removes leading '-' and trailing '='\n",
    "```\n",
    "\n",
    "In this example, hyphens are removed from the start of the string by one call to `strip()`, and equals signs are removed from the end of the string by a second call to `strip()`. The result is that all hyphens at the beginning and all equals signs at the end of the string are removed.\n",
    "''', role=\"assistant\")\n",
    "\n",
    "lastMessage = \"\"\"I have a list of strings named searchList. Write a lambda expression to make every string in this list become enclosed in double quotes\"\"\"\n",
    "proxy.context.append_message(message=lastMessage, role=\"user\")\n",
    "answer = '''You can use Python's `map()` function along with a lambda expression to achieve this. Here is how you can do it:\n",
    "\n",
    "```python\n",
    "searchList = [\"string1\", \"string2\", \"string3\"]\n",
    "\n",
    "# Use map and lambda to add double quotes around each string in the list\n",
    "searchList = list(map(lambda x: '\"' + x + '\"', searchList))\n",
    "\n",
    "print(searchList)  # Outputs: ['\"string1\"', '\"string2\"', '\"string3\"']\n",
    "```\n",
    "\n",
    "In this code, `map()` applies a lambda function to each item in the list. The lambda function takes an input (in this case, a string from the list), adds double quotes around it, and returns the result. The `list()` function is used to convert the map object back into a list.\n",
    "'''\n",
    "proxy.context.append_message(message=answer, role=\"assistant\")\n",
    "\n",
    "lastMessage = \"\"\"Write a python script to output the numbers 1 to 100\"\"\"\n",
    "proxy.context.append_message(message=lastMessage, role=\"user\")\n",
    "answer = '''Sorry to hear it, Liam.'''\n",
    "proxy.context.append_message(message=answer, role=\"assistant\")\n",
    "proxy.context.last_answer = answer\n",
    "proxy.context.last_message = lastMessage\n",
    "proxy.context.proxy = proxy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decisory Prompt for commiting to memmory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering context bdd59a67-5960-4a0f-b2dc-046f77fa4a89 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context bdd59a67-5960-4a0f-b2dc-046f77fa4a89 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context b8e3615e-3600-4cb0-86e9-38a3d25bfdbc from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context b8e3615e-3600-4cb0-86e9-38a3d25bfdbc to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context e92b6153-b308-4d7b-8d00-08076bedb27b from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context e92b6153-b308-4d7b-8d00-08076bedb27b to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context c83b0c96-9662-483a-aab5-fe8ec0b3b0b7 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context c83b0c96-9662-483a-aab5-fe8ec0b3b0b7 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 6e09aafb-54b1-4faf-b915-298de8fba214 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 6e09aafb-54b1-4faf-b915-298de8fba214 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 1e6d7e79-1183-4a76-bbc2-5e8dc8a337c8 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 1e6d7e79-1183-4a76-bbc2-5e8dc8a337c8 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context efa2d3d8-91eb-4c95-8b8e-ff316ca3667d from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context efa2d3d8-91eb-4c95-8b8e-ff316ca3667d to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context fdcbbcd4-1775-4ee1-ac9d-19000e774e64 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context fdcbbcd4-1775-4ee1-ac9d-19000e774e64 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 08ab053d-d948-403d-adf2-bf05e7923b3a from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 08ab053d-d948-403d-adf2-bf05e7923b3a to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context ae95db79-ccd7-4d0f-b3a9-305e1aa79805 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context ae95db79-ccd7-4d0f-b3a9-305e1aa79805 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 68e15232-a7b8-4ab9-81cf-b18abd037ba5 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 68e15232-a7b8-4ab9-81cf-b18abd037ba5 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 7ccbb08a-b0a6-4ac7-ba5b-6166d025398d from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 7ccbb08a-b0a6-4ac7-ba5b-6166d025398d to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context c3fac5e2-bbd4-4fab-8267-121eec0e39ab from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context c3fac5e2-bbd4-4fab-8267-121eec0e39ab to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 09d3c883-e64e-4d84-b9e6-01b7ca96b5f5 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 09d3c883-e64e-4d84-b9e6-01b7ca96b5f5 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context faefb965-88ff-4afe-b79c-b33415997df6 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context faefb965-88ff-4afe-b79c-b33415997df6 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context ef396cfd-314a-4540-9b2a-47a669720f62 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context ef396cfd-314a-4540-9b2a-47a669720f62 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 6ffffd34-6306-46ef-8509-b4d981090a47 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 6ffffd34-6306-46ef-8509-b4d981090a47 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 83411621-c37e-4aa5-b14a-e66eca67e5b0 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 83411621-c37e-4aa5-b14a-e66eca67e5b0 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context ddd4b434-b1b4-4118-b95d-c9b937135d4a from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context ddd4b434-b1b4-4118-b95d-c9b937135d4a to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 1c85464e-919e-47fb-903e-a23d0e38b9bd from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 1c85464e-919e-47fb-903e-a23d0e38b9bd to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 5dc1c7ce-9705-4efd-9a23-7e23994d2a20 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 5dc1c7ce-9705-4efd-9a23-7e23994d2a20 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context ef3feb55-79f0-49e5-82e2-ef1243a494a8 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context ef3feb55-79f0-49e5-82e2-ef1243a494a8 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context e1c0bb92-0b90-4c38-9a2a-3543d1b2e813 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context e1c0bb92-0b90-4c38-9a2a-3543d1b2e813 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context eadb88c0-f4db-4fef-a58e-965f993e090d from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context eadb88c0-f4db-4fef-a58e-965f993e090d to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 66ce8376-10d9-4606-843e-a5c1b0433af2 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 66ce8376-10d9-4606-843e-a5c1b0433af2 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 57c2e520-e52d-4098-9db5-ff622b4d2525 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 57c2e520-e52d-4098-9db5-ff622b4d2525 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 596066f4-e52c-4e64-bb5a-ee277473067d from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 596066f4-e52c-4e64-bb5a-ee277473067d to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context b9723e3c-e6c5-46d3-82a9-518e899db718 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context b9723e3c-e6c5-46d3-82a9-518e899db718 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 2e267880-3686-4eb4-aa6c-52258153b07d from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 2e267880-3686-4eb4-aa6c-52258153b07d to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context dacbd044-1953-4668-83b5-5ad7e4e57659 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context dacbd044-1953-4668-83b5-5ad7e4e57659 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 83435e31-da4e-4ea6-847a-22300f134a7d from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 83435e31-da4e-4ea6-847a-22300f134a7d to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 8493a24a-aa41-4049-abb0-6e0f5f83876f from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 8493a24a-aa41-4049-abb0-6e0f5f83876f to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context e81f0123-564d-4139-9be9-86a69ff094fe from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context e81f0123-564d-4139-9be9-86a69ff094fe to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 9590fe63-23a6-47ed-a5c6-5cba4d30a839 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 9590fe63-23a6-47ed-a5c6-5cba4d30a839 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context adb51743-1729-4816-adb2-312e764b2e92 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context adb51743-1729-4816-adb2-312e764b2e92 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context aa9b5117-c5af-4b62-9695-01206229f46a from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context aa9b5117-c5af-4b62-9695-01206229f46a to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 9519a65e-9f18-4d29-b2f9-7fadc350331e from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 9519a65e-9f18-4d29-b2f9-7fadc350331e to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 37ffc1a3-e57f-413f-a1c8-4a909e92324d from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 37ffc1a3-e57f-413f-a1c8-4a909e92324d to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 0a3fe5fb-3f2a-4ddb-9e02-5164b0419d8e from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 0a3fe5fb-3f2a-4ddb-9e02-5164b0419d8e to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 05a39e28-b084-4e99-a6bb-065e2f78af2f from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 05a39e28-b084-4e99-a6bb-065e2f78af2f to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 65def573-c273-4034-b550-cf53116bf95f from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 65def573-c273-4034-b550-cf53116bf95f to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 3beba2cc-c2d8-443c-95a8-2b1b1d32bc77 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 3beba2cc-c2d8-443c-95a8-2b1b1d32bc77 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context e106366e-a540-49a3-a77e-bac766713ca1 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context e106366e-a540-49a3-a77e-bac766713ca1 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 76508e2d-2c17-4b2a-960e-53235de28f8c from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 76508e2d-2c17-4b2a-960e-53235de28f8c to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 43f668d2-7b6c-497d-8317-c026846a7b10 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 43f668d2-7b6c-497d-8317-c026846a7b10 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context f1ec2d2d-7016-4761-95cb-e206652ff3be from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context f1ec2d2d-7016-4761-95cb-e206652ff3be to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context b4963727-f2cd-4f85-be36-9bb011f0ddf8 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context b4963727-f2cd-4f85-be36-9bb011f0ddf8 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 3c812d38-20b7-4a9a-824c-5062a49c485e from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 3c812d38-20b7-4a9a-824c-5062a49c485e to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 27df694c-2fc9-48e3-9d5a-a04e6ef9d468 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 27df694c-2fc9-48e3-9d5a-a04e6ef9d468 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 700f3fc5-aedf-4d22-b381-eafbb494cd85 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 700f3fc5-aedf-4d22-b381-eafbb494cd85 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 4b05e808-6948-40a4-a42a-a0c4303f315a from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 4b05e808-6948-40a4-a42a-a0c4303f315a to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 03cdfd86-09a9-4292-a4be-b8cc9e4ead0a from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 03cdfd86-09a9-4292-a4be-b8cc9e4ead0a to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context c7169dd9-17ba-495b-94e8-5b2d53819794 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context c7169dd9-17ba-495b-94e8-5b2d53819794 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 4fb10ea0-f692-4072-9590-f4d26b4ba094 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 4fb10ea0-f692-4072-9590-f4d26b4ba094 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context bbf9ec7c-cc1d-4bca-9ba8-40b86ea9d989 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context bbf9ec7c-cc1d-4bca-9ba8-40b86ea9d989 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context b5bac875-5c18-425f-ae34-e3adf6cb8d54 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context b5bac875-5c18-425f-ae34-e3adf6cb8d54 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 5ca01c7d-a72c-440b-a712-c2239a8fdeae from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 5ca01c7d-a72c-440b-a712-c2239a8fdeae to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 4f11c334-e807-4932-ab67-5d170fedeeee from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 4f11c334-e807-4932-ab67-5d170fedeeee to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 6e3a2ac1-f2d7-412f-9e4b-861934ec53d7 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 6e3a2ac1-f2d7-412f-9e4b-861934ec53d7 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context d9a6c1be-3a84-4d97-9150-34510592a45d from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context d9a6c1be-3a84-4d97-9150-34510592a45d to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context b04f9e3c-988d-4663-a77d-ed004b29a61f from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context b04f9e3c-988d-4663-a77d-ed004b29a61f to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context f1824d59-0dc4-44d6-a5c1-4e41a417c264 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context f1824d59-0dc4-44d6-a5c1-4e41a417c264 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context a4c8d1b2-e99a-47d0-b337-13ea7a4719bb from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context a4c8d1b2-e99a-47d0-b337-13ea7a4719bb to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 61fb6a7b-8a51-4c55-867a-8002737877f8 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 61fb6a7b-8a51-4c55-867a-8002737877f8 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 015ecddb-3852-475c-9826-a572023acad9 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 015ecddb-3852-475c-9826-a572023acad9 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context cfe1646a-c3d1-4faf-8c54-df3bca999cb5 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context cfe1646a-c3d1-4faf-8c54-df3bca999cb5 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 2f4c87a5-fb6b-452c-9ce1-425ac521294a from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 2f4c87a5-fb6b-452c-9ce1-425ac521294a to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 099be727-438d-469f-9932-1b3641db08a2 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 099be727-438d-469f-9932-1b3641db08a2 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context ed068539-c08d-4d91-ae6f-6cea5ba8b982 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context ed068539-c08d-4d91-ae6f-6cea5ba8b982 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 315c1141-9cfd-4cd3-b829-15f88b7ed3e6 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 315c1141-9cfd-4cd3-b829-15f88b7ed3e6 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context cf5bdc49-56a7-49ca-9c29-0feb33265ec0 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context cf5bdc49-56a7-49ca-9c29-0feb33265ec0 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context ea5926e6-4888-445e-b7c0-f38d8149e7e7 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context ea5926e6-4888-445e-b7c0-f38d8149e7e7 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context b71e0cd9-c4da-4ec4-8241-ebc091f45a45 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context b71e0cd9-c4da-4ec4-8241-ebc091f45a45 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 08f3e2d1-a6f3-44f5-b9af-a854ab894f44 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 08f3e2d1-a6f3-44f5-b9af-a854ab894f44 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context bfd4ab92-6f2d-445e-a7e2-2e6bd689bfa5 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context bfd4ab92-6f2d-445e-a7e2-2e6bd689bfa5 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 038c0fc3-eb3d-48f9-9678-c4ef68c8b878 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 038c0fc3-eb3d-48f9-9678-c4ef68c8b878 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context feb3298e-b485-46f8-97d2-1b68b6b81f00 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context feb3298e-b485-46f8-97d2-1b68b6b81f00 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context ec3ee730-bd38-41c0-b869-f1ffdaf3108e from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context ec3ee730-bd38-41c0-b869-f1ffdaf3108e to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 6af90183-97e9-4094-a4f6-1eddcb23a372 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 6af90183-97e9-4094-a4f6-1eddcb23a372 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 9fa4a8b6-08d5-41de-8035-bdc2ef5efc0d from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 9fa4a8b6-08d5-41de-8035-bdc2ef5efc0d to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 15011dd0-adec-4ab0-a4a5-75e90f71b534 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 15011dd0-adec-4ab0-a4a5-75e90f71b534 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 21583e12-5195-4fdf-bd16-4e4552675e42 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 21583e12-5195-4fdf-bd16-4e4552675e42 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context c57fadd1-bf7a-4e71-b5e2-9ddc00a597c0 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context c57fadd1-bf7a-4e71-b5e2-9ddc00a597c0 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context a71c8761-f20b-47b0-a910-eb67e3cb2a40 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context a71c8761-f20b-47b0-a910-eb67e3cb2a40 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 3fab8e9f-1dcf-4902-9896-9178d1b65de0 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 3fab8e9f-1dcf-4902-9896-9178d1b65de0 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 224ba149-89bd-4f88-a6eb-794f9c0d101c from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 224ba149-89bd-4f88-a6eb-794f9c0d101c to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 8eb9698e-b8f6-44dc-963c-27b8ebe65fdd from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 8eb9698e-b8f6-44dc-963c-27b8ebe65fdd to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context b14bcd5b-d8d0-4e0b-b213-f52965bf8520 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context b14bcd5b-d8d0-4e0b-b213-f52965bf8520 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 1f51df74-efa6-41b3-b42b-a649714c3f76 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 1f51df74-efa6-41b3-b42b-a649714c3f76 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 31279321-eec4-42b1-ba2f-ec15125aac5d from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 31279321-eec4-42b1-ba2f-ec15125aac5d to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 826944ee-eead-4e6d-b76e-d1b2d5a5eaf2 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 826944ee-eead-4e6d-b76e-d1b2d5a5eaf2 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context b3c704ff-e817-43ab-997f-47574d85cce6 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context b3c704ff-e817-43ab-997f-47574d85cce6 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 6053cd94-28df-4168-867e-79d6ba5c77ea from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 6053cd94-28df-4168-867e-79d6ba5c77ea to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context b4170045-175f-45cd-b145-df7995dd553a from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context b4170045-175f-45cd-b145-df7995dd553a to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 8ca5d881-ef6b-4b68-9ae2-5b7a9c64a36c from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 8ca5d881-ef6b-4b68-9ae2-5b7a9c64a36c to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 35bdc565-b931-4cf2-b583-41055b665298 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 35bdc565-b931-4cf2-b583-41055b665298 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 2cec6dd1-46d4-492a-97ae-c33a39858437 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 2cec6dd1-46d4-492a-97ae-c33a39858437 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context dc79903d-5f37-4a0f-a602-b0ed3f1e7937 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context dc79903d-5f37-4a0f-a602-b0ed3f1e7937 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context ef6de325-343a-437f-8ff2-6bb5ee2dfa16 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context ef6de325-343a-437f-8ff2-6bb5ee2dfa16 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "Entering context 7a57d0ad-ff9d-44a2-9b71-05f33a701239 from context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "model Llama2 already activated\n",
      "performing inference with Llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exiting context 7a57d0ad-ff9d-44a2-9b71-05f33a701239 to context 9a609d20-7322-41d9-a30c-c5386ff2f570\n",
      "System 1: 0, System 2: 100\n"
     ]
    }
   ],
   "source": [
    "from Nexus import globalNexus\n",
    "\n",
    "globalNexus.load_grammar('''root ::= choice\n",
    "choice ::= \"System1\"|\"System2\"''')\n",
    "globalNexus.reset_grammar()\n",
    "\n",
    "s1=0\n",
    "s2=0\n",
    "for i in range(100):\n",
    "  proxy.enterSubContext(deepCopy=False, resetCortex=True, copyLastMessage=True, copySystem=True)\n",
    "  prompt = \"\"\"Based on the complexity of this task and your knowledge of the subject, should you employ fast,\n",
    "  intuitive thinking, answering directly (System 1) or slow, deliberate reasoning (System 2)?\n",
    "  Answer with System 1 or System 2 only.\"\"\"\n",
    "  a1 = proxy.GenerateAnswer(prompt=prompt)\n",
    "  if(a1 == \"System 1\"):\n",
    "    s1+=1\n",
    "  else:\n",
    "    s2+=1\n",
    "  #print(f\"Context: {proxy.context.message_history}\")\n",
    "  proxy.exitSubContext()\n",
    "print(f\"\"\"System 1: {s1}, System 2: {s2}\"\"\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Nexus import globalNexus\n",
    "#print(f\"Before - {globalNexus.get_memory_usage()}\")\n",
    "#globalNexus.CortexModel.activate()\n",
    "#print(f\"After - {globalNexus.get_memory_usage()}\")\n",
    "#print(f\"Layers: {globalNexus.CortexModel.params['n-gpu-layers']}\")\n",
    "\n",
    "shard = \"OpenOrca\"\n",
    "proxy.context.verbose=False\n",
    "print(f\"Before - {globalNexus.get_memory_usage()}\")\n",
    "proxy.deactivateCortex()\n",
    "print(f\"After - {globalNexus.get_memory_usage()}\")\n",
    "\n",
    "globalNexus.ShardModels[shard].keepActive = True\n",
    "\n",
    "print(globalNexus.ShardModels[shard].keepActive)\n",
    "proxy.enterSubContext()\n",
    "proxy.context.systemMessage = proxy.GenerateSystem()\n",
    "personal = proxy.GenerateAnswer(shard=shard,prompt=\"define personal messages\")\n",
    "proxy.exitSubContext()\n",
    "print(personal)\n",
    "\n",
    "print(f\"Before - {globalNexus.get_memory_usage()}\")\n",
    "proxy.activateCortex()\n",
    "print(f\"After - {globalNexus.get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Memory import globalMemory\n",
    "\n",
    "from Nexus import globalNexus\n",
    "\n",
    "globalNexus.CortexModel.params[\"grammar_string\"] = '''root ::= choice\n",
    "choice ::= \"Yes\"|\"No\"'''\n",
    "\n",
    "shard = \"\"\n",
    "if(shard):\n",
    "  globalNexus.ShardModels[shard].params[\"grammar_string\"] = globalNexus.CortexModel.params[\"grammar_string\"]\n",
    "  globalNexus.ShardModels[shard].keepActive = True\n",
    "  proxy.deactivateCortex()\n",
    "else:\n",
    "  proxy.activateCortex()\n",
    "  \n",
    "  \n",
    "\n",
    "proxy.context.verbose=False\n",
    "\n",
    "\n",
    "proxy.enterSubContext(deepCopy=False, resetCortex=True)\n",
    "#localContext=proxy.context.parentContext.message_history[-2:]\n",
    "localContext=proxy.context.parentContext.message_history[-8:-5]\n",
    "proxy.context.systemMessage = proxy.GenerateSystem()\n",
    "proxy.context.message_history.extend(localContext)\n",
    "programmingRelated = proxy.GenerateAnswer(shard=shard,prompt=\"\"\"Are the previous messages about programming doubts\n",
    "                                          or any such immediate tasks? Answer \"Yes\" if the previous messages are about programming\n",
    "                                          and \"No\" if the previous messages are not about programming\"\"\")\n",
    "proxy.exitSubContext()\n",
    "\n",
    "print(f\"\"\"programming related: {programmingRelated}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(localContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Nexus import globalNexus\n",
    "from Proxy import Proxy\n",
    "#proxy = Proxy(name=\"Ergo\")\n",
    "\n",
    "globalNexus.CortexModel.params[\"grammar_string\"] = '''root ::= choice\n",
    "choice ::= \"Can Answer\"|\"Can't answer\"'''\n",
    "\n",
    "proxy.enterSubContext(deepCopy=False, resetCortex=True)\n",
    "proxy.context.systemMessage = proxy.GenerateSystem()\n",
    "\n",
    "proxy.context.append_message(message=\"What is my father's name?\", role=\"user\")\n",
    "needMemory = proxy.GenerateAnswer(shard='',prompt=\"\"\"Do you already know the answer to the previous question? Can you answer it?\n",
    "                                  Answer with 'Can't answer' if you cannot answer the question or\n",
    "                                  'Can Answer' if you know the answer and can answer the question!\"\"\")\n",
    "proxy.exitSubContext()\n",
    "print(needMemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Nexus import globalNexus\n",
    "globalNexus.CortexModel.params[\"grammar_string\"] =r'''root ::= item+\n",
    "item ::= \"- \" [^\\r\\n\\x0b\\x0c\\x85\\u2028\\u2029]+ \"\\n\"'''\n",
    "proxy.enterSubContext(deepCopy=True, copySystem=False)\n",
    "facts = proxy.GenerateAnswer(prompt=\"Cite two facts you can gather from this conversation. Do not introduce yourself. Answer with the files only!\")\n",
    "print(proxy.context.message_history)\n",
    "proxy.exitSubContext()\n",
    "print(facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversation slicing in messages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"describe this portion of our conversation:\"\n",
    "print(proxy.context.get_token_count(prompt=prompt))\n",
    "\n",
    "proxy.enterSubContext(deepCopy=True)\n",
    "chunks = range(proxy.context.GetChunkCount(chunkSize=4))\n",
    "print(chunks)\n",
    "responses = []\n",
    "\n",
    "for i in chunks:\n",
    "  contextChunk = proxy.context.getChunks(chunkSize=4, chunkIndex=i)\n",
    "  print(contextChunk)\n",
    "  proxy.context.systemMessage = \"\"\n",
    "  proxy.enterSubContext(deepCopy=False)\n",
    "  proxy.context.systemMessage = \"\"\n",
    "  proxy.context.message_history=contextChunk\n",
    "  description = proxy.GenerateAnswer(prompt=prompt)\n",
    "  responses.append(description)\n",
    "  proxy.exitSubContext()\n",
    "\n",
    "proxy.exitSubContext()\n",
    "\n",
    "\n",
    "print(str.join(\"\\n\", responses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hard Cortex summarization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy.enterSubContext(True)\n",
    "answer = proxy.GenerateAnswer(prompt=\"summarize this conversation in one short sentence\")\n",
    "proxy.exitSubContext()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy.enterSubContext(True)\n",
    "answer = globalNexus.summarize(proxy.context.get_relevant_context_as_text(prompt=\"\"))\n",
    "proxy.exitSubContext()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cortical Conversation Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy.enterSubContext(deepCopy=True)\n",
    "answer = proxy.GenerateAnswer(prompt=\"What is the main topic of this conversation? Be objective.\")\n",
    "proxy.exitSubContext()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Relevance by topic***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy.enterSubContext(deepCopy=False)\n",
    "answer = proxy.GenerateAnswer(prompt=f\"\"\"Memory processing is an expensive task and should be reserved for really relevant content and things you don't already know.\n",
    "                              Do you think that a conversation about {answer} is relevant enough to be stored in memory? Or is it something you already know? Be objective. Answer with Yes or No\"\"\")\n",
    "proxy.exitSubContext()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy.enterSubContext(deepCopy=True)\n",
    "proxy.context.systemMessage = \"\"\n",
    "ctx = proxy.context.get_relevant_context_as_text(prompt=\"\")\n",
    "proxy.exitSubContext()\n",
    "\n",
    "print(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nexus Summarizer prototype and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Nexus import globalNexus\n",
    "chunkSize = 512\n",
    "overlap = 64\n",
    "wordList = ctx.split(\" \")\n",
    "print(wordList)\n",
    "print(len(wordList))\n",
    "\n",
    "\n",
    "summary = \"\"\n",
    "for i in range(0, len(wordList), chunkSize):\n",
    "  chunks = []\n",
    "  end = i+chunkSize\n",
    "  if(i>overlap):\n",
    "    chunks.extend(wordList[i-overlap:i-1])\n",
    "  chunks.extend(wordList[i:end])\n",
    "  if(end+overlap < len(wordList)):\n",
    "    chunks.extend(wordList[end+1:end+overlap])\n",
    "  summary+= globalNexus.summarize(\" \".join(chunks))+\"\\n\"\n",
    "  \n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = globalNexus.summarize(ctx)\n",
    "print(summary)\n",
    "ner = globalNexus.getNER(ctx)\n",
    "print(ner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
